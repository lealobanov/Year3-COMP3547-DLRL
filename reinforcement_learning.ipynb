{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "cssg28_RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WTnECTvcIfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21be04a2-7ebe-44c5-e3cc-091f287ed826"
      },
      "source": [
        "import sys\n",
        "\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install gym\n",
        "\n",
        "import os\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "if not os.path.exists(\"segment_tree.py\"):\n",
        "    #Download the segment tree module which is needed for PER implementation\n",
        "    !wget https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n",
        "        \n",
        "from segment_tree import MinSegmentTree, SumSegmentTree"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5k1zOl_oHQz"
      },
      "source": [
        "Prioritized Memory Replay - extension from standard Replay Memory buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSZJAsjvcIfY"
      },
      "source": [
        "#https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/03.per.ipynb\n",
        "class PrioritizedReplayBuffer():\n",
        "    \n",
        "    def __init__(self, obs_dim, size, batch_size = 32,alpha = 0.6):\n",
        "\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(PrioritizedReplayBuffer, self).__init__()\n",
        "\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "        self.max_priority, self.tree_ptr = 1.0, 0\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # capacity must be positive and a power of 2.\n",
        "        tree_capacity = 1\n",
        "        while tree_capacity < self.max_size:\n",
        "            tree_capacity *= 2\n",
        "\n",
        "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
        "        self.min_tree = MinSegmentTree(tree_capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "      \n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "        self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
        "        self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
        "        self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
        "\n",
        "    def sample_batch(self, beta: float = 0.4):\n",
        "        \"\"\"Sample a batch of experiences.\"\"\"\n",
        "        \n",
        "        indices = self._sample_proportional()\n",
        "        \n",
        "        obs = self.obs_buf[indices]\n",
        "        next_obs = self.next_obs_buf[indices]\n",
        "        acts = self.acts_buf[indices]\n",
        "        rews = self.rews_buf[indices]\n",
        "        done = self.done_buf[indices]\n",
        "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
        "        \n",
        "        return dict(\n",
        "            obs=obs,\n",
        "            next_obs=next_obs,\n",
        "            acts=acts,\n",
        "            rews=rews,\n",
        "            done=done,\n",
        "            weights=weights,\n",
        "            indices=indices,\n",
        "        )\n",
        "        \n",
        "    def update_priorities(self, indices, priorities):\n",
        "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
        "\n",
        "\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.sum_tree[idx] = priority ** self.alpha\n",
        "            self.min_tree[idx] = priority ** self.alpha\n",
        "\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "            \n",
        "    def _sample_proportional(self):\n",
        "        \"\"\"Sample indices based on proportions.\"\"\"\n",
        "        indices = []\n",
        "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
        "        segment = p_total / self.batch_size\n",
        "        \n",
        "        for i in range(self.batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            upperbound = random.uniform(a, b)\n",
        "            idx = self.sum_tree.retrieve(upperbound)\n",
        "            indices.append(idx)\n",
        "            \n",
        "        return indices\n",
        "    \n",
        "    def _calculate_weight(self, idx, beta):\n",
        "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
        "        # get max weight\n",
        "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
        "        max_weight = (p_min * len(self)) ** (-beta)\n",
        "        \n",
        "        # calculate weights\n",
        "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
        "        weight = (p_sample * len(self)) ** (-beta)\n",
        "        weight = weight / max_weight\n",
        "        \n",
        "        return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfuDBA85RbTX"
      },
      "source": [
        "Dueling network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSwDPB94cIff"
      },
      "source": [
        "#https://github.com/higgsfield/RL-Adventure/blob/master/3.dueling%20dqn.ipynb\n",
        "class DuelingCNN_Network(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        super(DuelingCNN_Network, self).__init__()\n",
        "\n",
        "        # set common feature layer\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128), \n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        # set advantage layer\n",
        "        self.advantage_layer = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, out_dim),\n",
        "        )\n",
        "\n",
        "        # set value layer\n",
        "        self.value_layer = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.feature_layer(x)\n",
        "        \n",
        "        value = self.value_layer(feature)\n",
        "        advantage = self.advantage_layer(feature)\n",
        "\n",
        "        return value + advantage  - advantage.mean(dim=-1, keepdim=True)\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BTegKScRfCP"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4JD5J6RcIff"
      },
      "source": [
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, env, memory_size, batch_size, target_update, epsilon_decay, max_epsilon = 1.0, min_epsilon = 0.07, gamma = 0.99,\n",
        "\n",
        "        # PER parameters\n",
        "        alpha = 0.2,\n",
        "        beta = 0.6,\n",
        "        prior_eps = 1e-6,\n",
        "    ):\n",
        "       \n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "        \n",
        "        self.env = env\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        \n",
        "        # PER\n",
        "        self.beta = beta\n",
        "        self.prior_eps = prior_eps\n",
        "        self.memory = PrioritizedReplayBuffer(\n",
        "            obs_dim, memory_size, batch_size, alpha\n",
        "        )\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = DuelingCNN_Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target = DuelingCNN_Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "        \n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # epsilon greedy policy\n",
        "        if self.epsilon > np.random.random():\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = self.dqn(\n",
        "                torch.FloatTensor(state).to(self.device)\n",
        "            ).argmax()\n",
        "            selected_action = selected_action.detach().cpu().numpy()\n",
        "\n",
        "        self.transition = [state, selected_action]\n",
        "        \n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        self.transition += [reward, next_state, done]\n",
        "        self.memory.store(*self.transition)\n",
        "    \n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self):\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        # PER needs beta to calculate weights\n",
        "        samples = self.memory.sample_batch(self.beta)\n",
        "        weights = torch.FloatTensor(\n",
        "            samples[\"weights\"].reshape(-1, 1)\n",
        "        ).to(self.device)\n",
        "        indices = samples[\"indices\"]\n",
        "\n",
        "        # PER: importance sampling before average\n",
        "        elementwise_loss = self._compute_dqn_loss(samples)\n",
        "        loss = torch.mean(elementwise_loss * weights)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # PER: update priorities\n",
        "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
        "        new_priorities = loss_for_prior + self.prior_eps\n",
        "        self.memory.update_priorities(indices, new_priorities)\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    #Main training loop    \n",
        "    def train(self, num_frames):\n",
        "        #Printing episode data to console \n",
        "        print_every = 1\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        update_cnt = 0\n",
        "        scores = []\n",
        "        marking  = []\n",
        "        score = 0\n",
        "        episode_count = 0 \n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            \n",
        "            # PER: increase beta\n",
        "            fraction = min(frame_idx / num_frames, 1.0)\n",
        "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
        "\n",
        "            #When episode is completed log scores and reset variables\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                scores.append(score)\n",
        "                marking.append(score)\n",
        "\n",
        "                if episode_count%100 == 0:\n",
        "                  print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
        "                  episode_count, score, np.array(marking).mean(), np.array(marking).std()))\n",
        "                  marking = []\n",
        "\n",
        "                if episode_count%print_every==0 and episode_count!=0:\n",
        "                  print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(episode_count, score, self.epsilon))\n",
        "                \n",
        "                score = 0\n",
        "                episode_count +=1\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                update_cnt += 1\n",
        "                \n",
        "                #Decrease epsilon\n",
        "                self.update_epsilon()\n",
        "\n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "                          \n",
        "        self.env.close()\n",
        "    \n",
        "    #Whether to pursue exploration\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(\n",
        "                      self.min_epsilon, self.epsilon - (\n",
        "                          self.max_epsilon - self.min_epsilon\n",
        "                      ) * self.epsilon_decay\n",
        "                  )\n",
        "    \n",
        "    def _compute_dqn_loss(self, samples):       \n",
        "\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(self.device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(self.device)\n",
        "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(self.device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(self.device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(self.device)\n",
        "\n",
        "        # Make predictions\n",
        "        state_q_values = self.dqn(state)\n",
        "        next_states_q_values = self.dqn_target(next_state)\n",
        "\n",
        "        #Double DQN - updating the DQN loss target as per https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/02.double_q.ipynb\n",
        "        curr_q_value = state_q_values.gather(1, action)\n",
        "        next_q_value = next_states_q_values.gather(1, self.dqn(next_state).argmax(dim=1, keepdim=True)).detach()\n",
        "        target = (reward + self.gamma * next_q_value * (1 - done)).to(self.device)\n",
        "\n",
        "        # calculate element-wise dqn loss\n",
        "        elementwise_loss = F.smooth_l1_loss(curr_q_value, target, reduction=\"none\")\n",
        "\n",
        "        return elementwise_loss \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Rpulp9cIfg"
      },
      "source": [
        "Set up environment and random seed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmqnulNcIfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1752694-064a-437a-9f0c-1009cf746a4e"
      },
      "source": [
        "#Configure environment\n",
        "env_id = \"Gravitar-ram-v0\"\n",
        "env = gym.make(env_id)\n",
        "\n",
        "video_every = 3 #Frequent videos to collect performance results; however this siginificantly slows the run time\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
        "\n",
        "#Reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\n",
        "seed = 742\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[742]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHPF8M2PcIfj"
      },
      "source": [
        "Set parameters and begin training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6ncOm_AcIfi"
      },
      "source": [
        "#Set parameters\n",
        "num_frames = 2500000 #An episode averages ~1000 frames; training for approx. 2500 episodes until a Google Colab timeout is encountered\n",
        "memory_size = 100000\n",
        "batch_size = 32\n",
        "target_update = 100\n",
        "#Epsilon initially decays slowly so as to encourage exploration of alternative game strategies\n",
        "epsilon_decay = 1/10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOJ7CoHScIfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "057614fc-6565-4fd4-be91-697630046163"
      },
      "source": [
        "#Begin training the agent\n",
        "agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay)\n",
        "agent.train(num_frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-31cd7713c5ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Begin training the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-79a4c201ab69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_frames)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# if training is ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-79a4c201ab69>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# PER: importance sampling before average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0melementwise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dqn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melementwise_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-79a4c201ab69>\u001b[0m in \u001b[0;36m_compute_dqn_loss\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mstate_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mnext_states_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-63c5c79cde4a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 128, 8, 8], but got 2-dimensional input of size [32, 128] instead"
          ]
        }
      ]
    }
  ]
}